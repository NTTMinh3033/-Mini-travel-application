{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install fastapi uvicorn pydantic requests","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!npm install -g localtunnel","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\n\nOLLAMA_MODEL = \"llama3:8b-instruct-q4_0\"\nprint(\"1. Installing Ollama...\")\nos.system('curl -fsSL https://ollama.com/install.sh | sh')\n\nprint(\"2. Starting Ollama daemon in background...\")\nos.system('nohup ollama serve &') \n\ntime.sleep(5)\nprint(f\"3. Pulling model: {OLLAMA_MODEL}...\")\nos.system(f'ollama pull {OLLAMA_MODEL}')\n\ntime.sleep(5)\n\nprint(\"Ollama setup complete. Service should be running on http://localhost:11434.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile main.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List\nimport requests\nimport json\nfrom datetime import date\n\n# Khai báo Ứng dụng và Cấu hình\n\napp = FastAPI(title=\"AI Travel Itinerary Backend\")\n\nOLLAMA_ENDPOINT = \"http://localhost:11434/api/generate\"\nOLLAMA_MODEL = \"llama3:8b-instruct-q4_0\" \n\n# Khai báo Mô hình Dữ liệu (Pydantic)\n\nclass ItineraryRequest(BaseModel):\n    origin: str\n    destination: str\n    start_date: date\n    end_date: date\n    interests: List[str]\n    pace: str\n\nclass ItineraryResponse(BaseModel):\n    status: str\n    message: str\n    itinerary: dict = None\n\n# Hàm gọi LLM và Xử lý Prompt\n\ndef generate_llm_prompt(data: ItineraryRequest) -> str:\n    \n    num_days = (data.end_date - data.start_date).days + 1\n    interests_str = \", \".join(data.interests)\n    \n    prompt = f\"\"\"\n    Bạn là một trợ lý du lịch AI. Nhiệm vụ của bạn là tạo ra một hành trình chi tiết.\n    \n    Yêu cầu chuyến đi:\n    - Điểm đi: {data.origin}\n    - Điểm đến: {data.destination}\n    - Ngày: Từ {data.start_date} đến {data.end_date} (Tổng cộng {num_days} ngày)\n    - Sở thích: {interests_str}\n    - Tốc độ: {data.pace}\n    \n    Định dạng đầu ra:\n    Bạn phải trả lời bằng định dạng JSON hợp lệ, KHÔNG CHỨA bất kỳ ký tự hoặc giải thích nào nào khác ngoài JSON.\n    Cấu trúc JSON PHẢI tuân theo schema sau:\n    {{\n        \"itinerary\": [\n            {{\n                \"day\": 1,\n                \"date\": \"YYYY-MM-DD\",\n                \"activities\": [\n                    {{\n                        \"time\": \"Sáng\",\n                        \"place\": \"Tên địa điểm\",\n                        \"explanation\": \"Giải thích ngắn gọn (Vd: Tham quan, ăn uống)\"\n                    }},\n                    // ... các hoạt động khác: \"Trưa\", \"Chiều\", \"Tối\"\n                ]\n            }},\n            // ... các ngày khác\n        ]\n    }}\n    Bắt đầu tạo hành trình ngay bây giờ:\n    \"\"\"\n    return prompt\n\n# Endpoint API\n\n@app.post(\"/generate_itinerary\", response_model=ItineraryResponse)\nasync def generate_itinerary(data: ItineraryRequest):\n    \"\"\"Xử lý yêu cầu từ Frontend và gọi LLM thông qua Ollama.\"\"\"\n    \n    full_prompt = generate_llm_prompt(data)\n    \n    ollama_payload = {\n        \"model\": OLLAMA_MODEL,\n        \"prompt\": full_prompt,\n        \"format\": \"json\", \n        \"stream\": False\n    }\n\n    try:\n        response = requests.post(\n            OLLAMA_ENDPOINT, \n            json=ollama_payload, \n            timeout=300\n        )\n        response.raise_for_status()\n        \n        ollama_response = response.json()\n        raw_output = ollama_response.get(\"response\", \"{}\")\n        \n        try:\n            itinerary_json = json.loads(raw_output)\n            \n            return ItineraryResponse(\n                status=\"success\",\n                message=\"Hành trình được tạo thành công.\",\n                itinerary=itinerary_json\n            )\n        except json.JSONDecodeError as e:\n            return ItineraryResponse(\n                status=\"error\",\n                message=f\"LLM trả về JSON không hợp lệ. Lỗi: {e}\",\n                itinerary={\"raw_output\": raw_output}\n            )\n\n    except requests.exceptions.RequestException as e:\n        raise HTTPException(\n            status_code=503, \n            detail=f\"Lỗi kết nối đến Ollama Backend: {e}. Vui lòng đảm bảo Ollama đang chạy trên cổng 11434.\"\n        )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dọn dẹp cổng\n!fuser -k 8005/tcp\n\nimport subprocess\nimport time\nimport os\nimport signal\n\nFASTAPI_PORT = 8005\n\nprint(f\"Starting FastAPI server on port {FASTAPI_PORT} using subprocess...\")\n\ntry:\n    uvicorn_process = subprocess.Popen(\n        ['uvicorn', 'main:app', '--host', '0.0.0.0', '--port', str(FASTAPI_PORT)],\n        preexec_fn=os.setsid \n    )\n    print(f\"FastAPI is running (PID: {uvicorn_process.pid}).\")\nexcept Exception as e:\n    print(f\"LỖI KHỞI ĐỘNG UVICORN: {e}\")\n    raise\n\ntime.sleep(5)\n\nprint(\"Starting LocalTunnel...\")\nprint(\"Vui lòng truy cập URL hiển thị dưới đây:\")\nget_ipython().system(f'lt --port {FASTAPI_PORT}')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}