{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# ğŸ§© Colab All-in-One: Ollama + FastAPI + Streamlit + ngrok (âœ… báº£n fix lá»—i 403)\n",
        "# ==========================================\n",
        "\n",
        "!apt-get install -y curl > /dev/null\n",
        "!pip install fastapi uvicorn nest-asyncio pyngrok streamlit requests > /dev/null\n",
        "\n",
        "import os, json, time, subprocess, threading, requests, nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# ------------------------\n",
        "# 1ï¸âƒ£ CÃ i vÃ  khá»Ÿi Ä‘á»™ng Ollama\n",
        "# ------------------------\n",
        "print(\"ğŸš€ CÃ i Ä‘áº·t Ollama...\")\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "print(\"\\nâš™ï¸ Khá»Ÿi Ä‘á»™ng Ollama server...\")\n",
        "ollama_proc = subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(10)\n",
        "\n",
        "print(\"ğŸ§  Kiá»ƒm tra model gemma:2b...\")\n",
        "!ollama pull gemma:2b || echo \"ÄÃ£ cÃ³ model gemma:2b\"\n",
        "\n",
        "# ------------------------\n",
        "# 2ï¸âƒ£ Viáº¿t FastAPI app (gá»i Ollama API ná»™i bá»™)\n",
        "# ------------------------\n",
        "app_code = \"\"\"\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Dict\n",
        "import requests, json\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Cho phÃ©p truy cáº­p tá»« báº¥t ká»³ nguá»“n nÃ o (trÃ¡nh lá»—i CORS)\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "class ItineraryRequest(BaseModel):\n",
        "    username: str\n",
        "    origin: str\n",
        "    destination: str\n",
        "    dates: Dict[str, str]\n",
        "    interests: List[str]\n",
        "    pace: str\n",
        "\n",
        "def generate_itinerary(prompt: str):\n",
        "    url = \"http://127.0.0.1:11434/api/generate\"\n",
        "    data = {\"model\": \"gemma:2b\", \"prompt\": prompt}\n",
        "    try:\n",
        "        response = requests.post(url, json=data, timeout=120)\n",
        "        if response.status_code == 200:\n",
        "            # GhÃ©p ná»™i dung tá»« cÃ¡c dÃ²ng streaming\n",
        "            lines = [json.loads(l)[\"response\"] for l in response.text.splitlines() if '\"response\"' in l]\n",
        "            return \"\".join(lines)\n",
        "        else:\n",
        "            return f\"Error: Ollama returned {response.status_code}: {response.text[:200]}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error calling Ollama: {e}\"\n",
        "\n",
        "@app.post(\"/generate_itinerary\")\n",
        "def create_itinerary(req: ItineraryRequest):\n",
        "    prompt = f'''\n",
        "You are a travel planner AI. Create a day-by-day itinerary from {req.origin} to {req.destination}\n",
        "from {req.dates['start']} to {req.dates['end']}.\n",
        "Focus on interests: {', '.join(req.interests)}.\n",
        "Use pace: {req.pace}.\n",
        "Divide each day into morning, afternoon, and evening with 1â€“2 sentences each.\n",
        "Return JSON format like:\n",
        "{{\"day 1\": {{\"morning\": \"...\", \"afternoon\": \"...\", \"evening\": \"...\"}}}}\n",
        "'''\n",
        "    result = generate_itinerary(prompt)\n",
        "    return {\"itinerary\": result}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(app_code)\n",
        "print(\"âœ… FastAPI app.py Ä‘Ã£ Ä‘Æ°á»£c táº¡o.\")\n",
        "\n",
        "# ------------------------\n",
        "# 3ï¸âƒ£ Cháº¡y FastAPI server (ná»™i bá»™)\n",
        "# ------------------------\n",
        "def run_fastapi():\n",
        "    os.system(\"uvicorn app:app --host 0.0.0.0 --port 8000 --log-level warning\")\n",
        "\n",
        "fastapi_thread = threading.Thread(target=run_fastapi, daemon=True)\n",
        "fastapi_thread.start()\n",
        "time.sleep(8)\n",
        "print(\"âœ… FastAPI server Ä‘Ã£ khá»Ÿi Ä‘á»™ng trÃªn localhost:8000.\")\n",
        "\n",
        "# ------------------------\n",
        "# 4ï¸âƒ£ Táº¡o Streamlit UI (frontend)\n",
        "# ------------------------\n",
        "streamlit_code = \"\"\"\n",
        "import streamlit as st\n",
        "import requests\n",
        "import json\n",
        "\n",
        "st.set_page_config(page_title=\"Travel Itinerary Planner\", page_icon=\"ğŸŒ\", layout=\"centered\")\n",
        "\n",
        "st.title(\"ğŸŒ Travel Itinerary Planner\")\n",
        "st.write(\"Nháº­p thÃ´ng tin chuyáº¿n Ä‘i cá»§a báº¡n vÃ  nháº­n káº¿ hoáº¡ch chi tiáº¿t!\")\n",
        "\n",
        "username = st.text_input(\"ğŸ‘¤ TÃªn ngÆ°á»i dÃ¹ng\", \"guest\")\n",
        "origin = st.text_input(\"ğŸ™ï¸ Äiá»ƒm khá»Ÿi hÃ nh\", \"Tokyo\")\n",
        "destination = st.text_input(\"ğŸ¯ Äiá»ƒm Ä‘áº¿n\", \"Kyoto\")\n",
        "start_date = st.date_input(\"ğŸ“… NgÃ y báº¯t Ä‘áº§u\")\n",
        "end_date = st.date_input(\"ğŸ“… NgÃ y káº¿t thÃºc\")\n",
        "interests = st.multiselect(\"ğŸ’¡ Sá»Ÿ thÃ­ch\", [\"food\", \"museums\", \"nature\", \"nightlife\"], [\"food\", \"nature\"])\n",
        "pace = st.selectbox(\"â±ï¸ Tá»‘c Ä‘á»™ hÃ nh trÃ¬nh\", [\"relaxed\", \"normal\", \"tight\"], index=1)\n",
        "\n",
        "if st.button(\"âœ¨ Táº¡o lá»‹ch trÃ¬nh\"):\n",
        "    if not username or not origin or not destination:\n",
        "        st.error(\"âš ï¸ Vui lÃ²ng nháº­p Ä‘áº§y Ä‘á»§ thÃ´ng tin.\")\n",
        "    else:\n",
        "        payload = {\n",
        "            \"username\": username,\n",
        "            \"origin\": origin,\n",
        "            \"destination\": destination,\n",
        "            \"dates\": {\n",
        "                \"start\": str(start_date),\n",
        "                \"end\": str(end_date)\n",
        "            },\n",
        "            \"interests\": interests,\n",
        "            \"pace\": pace\n",
        "        }\n",
        "        st.info(\"â³ Äang gá»­i yÃªu cáº§u tá»›i FastAPI server...\")\n",
        "        try:\n",
        "            # Gá»i trá»±c tiáº¿p FastAPI ná»™i bá»™\n",
        "            res = requests.post(\"http://localhost:8000/generate_itinerary\", json=payload, timeout=180)\n",
        "            if res.status_code == 200:\n",
        "                data = res.json()\n",
        "                st.success(\"âœ… Lá»‹ch trÃ¬nh Ä‘Ã£ sáºµn sÃ ng!\")\n",
        "                st.text_area(\"ğŸ“œ Káº¿t quáº£\", data[\"itinerary\"], height=400)\n",
        "            else:\n",
        "                st.error(f\"âŒ Lá»—i: mÃ£ tráº£ vá» {res.status_code}\")\n",
        "                st.text(res.text[:500])\n",
        "        except Exception as e:\n",
        "            st.error(f\"ğŸš« Lá»—i káº¿t ná»‘i: {e}\")\n",
        "\"\"\"\n",
        "\n",
        "with open(\"streamlit_app.py\", \"w\") as f:\n",
        "    f.write(streamlit_code)\n",
        "print(\"âœ… ÄÃ£ táº¡o streamlit_app.py (gá»i ná»™i bá»™ localhost).\")\n",
        "\n",
        "# ------------------------\n",
        "# 5ï¸âƒ£ Cháº¡y Streamlit vÃ  má»Ÿ ngrok\n",
        "# ------------------------\n",
        "print(\"ğŸš€ Khá»Ÿi Ä‘á»™ng Streamlit server...\")\n",
        "streamlit_proc = subprocess.Popen(\n",
        "    [\"streamlit\", \"run\", \"streamlit_app.py\", \"--server.port\", \"8501\", \"--server.address\", \"0.0.0.0\"]\n",
        ")\n",
        "time.sleep(10)\n",
        "\n",
        "# âš ï¸ Thay báº±ng token ngrok cá»§a báº¡n\n",
        "ngrok_token = input(\"Nháº­p Ngrok Authtoken (https://dashboard.ngrok.com/get-started/your-authtoken): \").strip()\n",
        "if ngrok_token:\n",
        "    ngrok.set_auth_token(ngrok_token)\n",
        "\n",
        "streamlit_url = ngrok.connect(8501)\n",
        "print(f\"ğŸ“º Má»Ÿ giao diá»‡n Streamlit táº¡i: {streamlit_url.public_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jlgEmMb-j8P",
        "outputId": "30e05f5c-a2b1-42e3-d12c-545f9bbe4443"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ CÃ i Ä‘áº·t Ollama...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\n",
            "âš™ï¸ Khá»Ÿi Ä‘á»™ng Ollama server...\n",
            "ğŸ§  Kiá»ƒm tra model gemma:2b...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "âœ… FastAPI app.py Ä‘Ã£ Ä‘Æ°á»£c táº¡o.\n",
            "âœ… FastAPI server Ä‘Ã£ khá»Ÿi Ä‘á»™ng trÃªn localhost:8000.\n",
            "âœ… ÄÃ£ táº¡o streamlit_app.py (gá»i ná»™i bá»™ localhost).\n",
            "ğŸš€ Khá»Ÿi Ä‘á»™ng Streamlit server...\n",
            "Nháº­p Ngrok Authtoken (https://dashboard.ngrok.com/get-started/your-authtoken): 35CVW7zt8dgrG1xqWW59CEafYfQ_59Db3VwDy2qi5HxmF2BtK\n",
            "ğŸ“º Má»Ÿ giao diá»‡n Streamlit táº¡i: https://jalen-prandial-imitatively.ngrok-free.dev\n"
          ]
        }
      ]
    }
  ]
}